# Edge-Model-Infra

ğŸš§ **å¼€å‘ä¸­** - ç”¨äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰éƒ¨ç½²å’Œç®¡ç†çš„åˆ†å¸ƒå¼è¾¹ç¼˜è®¡ç®—åŸºç¡€è®¾æ–½

## ğŸ“‹ é¡¹ç›®çŠ¶æ€

**å½“å‰ç‰ˆæœ¬**: v0.3.0-alpha
**å¼€å‘é˜¶æ®µ**: æ ¸å¿ƒæ¶æ„å®Œæˆï¼Œå¼€å§‹LLMé›†æˆå’ŒCUDAä¼˜åŒ–
**æœ€åæ›´æ–°**: 2024å¹´12æœˆ
**å¼€å‘è€…**: gugugu5331

## ğŸ¯ é¡¹ç›®ç›®æ ‡

æ„å»ºé«˜æ€§èƒ½ã€å¯æ‰©å±•çš„è¾¹ç¼˜LLMæ¨ç†åŸºç¡€è®¾æ–½ï¼Œæ”¯æŒï¼š

- åˆ†å¸ƒå¼æ¨¡å‹éƒ¨ç½²
- é«˜æ•ˆé€šä¿¡åè®®
- GPUåŠ é€Ÿæ¨ç†
- å®¹å™¨åŒ–éƒ¨ç½²

## ğŸ—ï¸ å¼€å‘è¿›åº¦

- [x] é¡¹ç›®åˆå§‹åŒ–
- [x] åŸºç¡€å·¥å…·åº“å¼€å‘ (JSONå¤„ç†ã€æ—¥å¿—ç³»ç»Ÿ)
- [x] Dockerç¯å¢ƒé…ç½®
- [x] æ„å»ºç³»ç»Ÿæ­å»º
- [x] ç½‘ç»œé€šä¿¡å±‚å®ç° (TCPæœåŠ¡å™¨ã€äº‹ä»¶å¾ªç¯ã€çº¿ç¨‹æ± )
- [x] æ¶ˆæ¯åè®®è®¾è®¡ (ZeroMQã€åºåˆ—åŒ–)
- [x] StackFlowäº‹ä»¶é©±åŠ¨æ¡†æ¶
- [x] å•å…ƒç®¡ç†å™¨æ ¸å¿ƒæ¶æ„
- [ ] LLMé›†æˆæ¥å£
- [ ] CUDAä¼˜åŒ–æ”¯æŒ

## ğŸ“ æœ€è¿‘æ›´æ–°

- **2024-10-18**: å®Œæˆç½‘ç»œå±‚å•å…ƒæµ‹è¯•å’Œå‹åŠ›æµ‹è¯•
- **2024-10-16**: ä¼˜åŒ–ç½‘ç»œI/Oæ€§èƒ½å’Œç¼“å†²åŒºç®¡ç†
- **2024-10-14**: å®ç°å¤šçº¿ç¨‹äº‹ä»¶å¾ªç¯å’Œçº¿ç¨‹æ± 
- **2024-10-11**: ä¿®å¤TCPè¿æ¥å†…å­˜æ³„æ¼é—®é¢˜
- **2024-10-09**: å®ŒæˆTCPæœåŠ¡å™¨å’Œè¿æ¥ç®¡ç†
- **2024-10-05**: å®ç°äº‹ä»¶é©±åŠ¨æ¶æ„(epollæ”¯æŒ)
- **2024-10-01**: åˆå§‹åŒ–ç½‘ç»œå±‚åŸºç¡€æ¶æ„

## ğŸ”§ ç½‘ç»œå±‚ç‰¹æ€§

- **é«˜æ€§èƒ½I/O**: åŸºäºepollçš„äº‹ä»¶é©±åŠ¨æ¶æ„
- **å¤šçº¿ç¨‹æ”¯æŒ**: çº¿ç¨‹æ± å’Œå¤šçº¿ç¨‹äº‹ä»¶å¾ªç¯
- **è¿æ¥ç®¡ç†**: TCPæœåŠ¡å™¨å’Œå®¢æˆ·ç«¯è¿æ¥ç®¡ç†
- **ç¼“å†²åŒºä¼˜åŒ–**: é«˜æ•ˆçš„ç½‘ç»œæ•°æ®ç¼“å†²å’Œå¤„ç†
- **è°ƒè¯•æ”¯æŒ**: å®Œæ•´çš„è°ƒè¯•æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§

## ğŸš€ è®¡åˆ’åŠŸèƒ½

- **åˆ†å¸ƒå¼æ¶æ„**: æ¨¡å—åŒ–è®¾è®¡ï¼Œç»„ä»¶ç‹¬ç«‹
- **LLMé›†æˆ**: å†…ç½®æ¨¡å‹éƒ¨ç½²å’Œæ¨ç†æ”¯æŒ
- **é«˜æ€§èƒ½é€šä¿¡**: åŸºäºZeroMQçš„æ¶ˆæ¯ç³»ç»Ÿ
- **Dockeræ”¯æŒ**: å®¹å™¨åŒ–éƒ¨ç½²æ–¹æ¡ˆ
- **äº‹ä»¶é©±åŠ¨**: å¼‚æ­¥äº‹ä»¶å¤„ç†æ¶æ„

## ğŸ“ Project Structure

```
Edge-Model-Infra/
â”œâ”€â”€ infra-controller/     # Infrastructure control and flow management
â”œâ”€â”€ unit-manager/         # Core unit management and coordination
â”œâ”€â”€ network/             # Network communication layer
â”œâ”€â”€ hybrid-comm/         # Hybrid communication protocols (ZMQ wrapper)
â”œâ”€â”€ node/               # Node management and LLM integration
â”œâ”€â”€ sample/             # Example implementations and test clients
â”œâ”€â”€ docker/             # Docker configuration and build scripts
â”œâ”€â”€ utils/              # Utility libraries and helpers
â””â”€â”€ thirds/             # Third-party dependencies
```

## ğŸ› ï¸ Components

### Infrastructure Controller (`infra-controller/`)
- **StackFlow**: Event-driven workflow management system
- **Channel Management**: Communication channel abstraction
- **Flow Control**: Request/response flow coordination

### Unit Manager (`unit-manager/`)
- **Core Service**: Main service orchestration (`main.cpp`)
- **Remote Actions**: RPC-style action handling
- **Session Management**: Client session lifecycle management
- **ZMQ Bus**: Message bus implementation
- **TCP Communication**: HTTP-like TCP server for external APIs

### Network Layer (`network/`)
- **Event Loop**: High-performance event-driven networking
- **TCP Server/Client**: Robust TCP communication
- **Connection Management**: Connection pooling and lifecycle management
- **Buffer Management**: Efficient data buffering

### Hybrid Communication (`hybrid-comm/`)
- **pzmq**: ZeroMQ wrapper with enhanced functionality
- **Message Serialization**: Efficient data serialization/deserialization
- **Protocol Abstraction**: Unified communication interface

## ğŸ”§ Prerequisites

- **OS**: Ubuntu 20.04 or compatible Linux distribution
- **Compiler**: GCC/G++ with C++17 support
- **CMake**: Version 3.10 or higher
- **Dependencies**:
  - libzmq3-dev
  - libgoogle-glog-dev
  - libboost-all-dev
  - libssl-dev
  - libbsd-dev
  - eventpp
  - simdjson

## ğŸš€ Quick Start

### 1. Clone the Repository
```bash
git clone https://github.com/gugugu5331/Edge-Model-Infra.git
cd Edge-Model-Infra
```

### 2. Build with Docker (Recommended)
```bash
# Build the Docker image
cd docker/scripts
./llm_docker_run.sh

# Enter the container
./llm_docker_into.sh
```

### 3. Manual Build
```bash
# Install dependencies
sudo ./build.sh

# Build the project
mkdir build && cd build
cmake ..
make -j$(nproc)
```

### 4. Run the System
```bash
# Start the unit manager
cd unit-manager
./unit_manager

# The system will start on port 10001 (configurable in master_config.json)
```

## ğŸ“– Usage Examples

### Python Client Example
```python
import socket
import json

# Connect to the service
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
sock.connect(('localhost', 10001))

# Setup LLM
setup_data = {
    "request_id": "llm_001",
    "work_id": "llm",
    "action": "setup",
    "object": "llm.setup",
    "data": {
        "model": "DeepSeek-R1-Distill-Qwen-1.5B",
        "response_format": "llm.utf-8.stream",
        "max_token_len": 1023,
        "prompt": "You are a helpful assistant."
    }
}

# Send setup request
sock.sendall((json.dumps(setup_data) + '\n').encode('utf-8'))
response = sock.recv(4096).decode('utf-8')
print("Setup response:", response)
```

### C++ RPC Example
```cpp
#include "pzmq.hpp"
using namespace StackFlows;

// Create RPC server
pzmq rpc_server("my_service");
rpc_server.register_rpc_action("process", [](pzmq* self, const std::shared_ptr<pzmq_data>& msg) {
    return "Processed: " + msg->string();
});

// Create RPC client
pzmq rpc_client("client");
auto result = rpc_client.rpc_call("my_service", "process", "Hello World");
```

## âš™ï¸ Configuration

### Unit Manager Configuration (`unit-manager/master_config.json`)
```json
{
    "config_tcp_server": 10001,
    "config_zmq_min_port": 5010,
    "config_zmq_max_port": 5555,
    "config_zmq_s_format": "ipc:///tmp/llm/%i.sock",
    "config_zmq_c_format": "ipc:///tmp/llm/%i.sock"
}
```

## ğŸ§ª Testing

Run the included test client:
```bash
cd sample
python3 test.py --host localhost --port 10001
```

Run C++ samples:
```bash
cd sample
# Terminal 1: Start RPC server
./rpc_server

# Terminal 2: Run RPC client
./rpc_call
```

## ğŸ³ Docker Deployment

The project includes comprehensive Docker support:

```bash
# Build and run with Docker
cd docker/scripts
./llm_docker_run.sh    # Build and start container
./llm_docker_into.sh   # Enter running container
```

## ğŸ“Š API Reference

### Setup LLM Model
```json
POST /
{
    "request_id": "unique_id",
    "work_id": "llm",
    "action": "setup",
    "object": "llm.setup",
    "data": {
        "model": "model_name",
        "response_format": "llm.utf-8.stream",
        "max_token_len": 1023
    }
}
```

### Inference Request
```json
POST /
{
    "request_id": "unique_id",
    "work_id": "returned_work_id",
    "action": "inference",
    "object": "llm.utf-8.stream",
    "data": {
        "delta": "user_input",
        "index": 0,
        "finish": true
    }
}
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¢ Copyright

Copyright Â© 2024 M5Stack Technology CO LTD

## ğŸ”— Links

- [GitHub Repository](https://github.com/gugugu5331/Edge-Model-Infra)
- [Issues](https://github.com/gugugu5331/Edge-Model-Infra/issues)
- [Pull Requests](https://github.com/gugugu5331/Edge-Model-Infra/pulls)

## ğŸ“ Support

For support and questions, please open an issue on GitHub or contact the development team.

---

**Note**: This project is actively under development. Some features may be experimental or subject to change.
